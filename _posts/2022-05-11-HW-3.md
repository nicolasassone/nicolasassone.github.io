---
layout: post
title: Detecting Cats and Dogs through CNNs
---

In this post, we will train and test several four convolutional neural networks (CNNs) on a dataset containing various images of cats and dogs with the purpose of classifying an image as containing a cat or dog. 

We begin with our necessary imports:

```python
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
```

Next, we run the following code to import our dataset containing the images of cats and dogs that we will be training and testing on.

We also split this dataset up into the training data, validation data, and testing data

```python
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
```

Next is a function which allows us to extract the actual images of three ranndomly selected cats and dogs. We run it on the training_data to get the following plot

```python
def view_cats_dogs(train_dataset):

    class_names = train_dataset.class_names

    plt.figure(figsize=(10, 10))


    cat_nums=[]
    dog_nums=[]
    for images, labels in train_dataset.take(1):
        for i in range(0,32):
            if str(class_names[labels[i]])=='cats':
                cat_nums.append(i)
            else:
                dog_nums.append(i)
            if (len(cat_nums) == 3 and len(dog_nums)==3):
                break
            
        for i in range(3):
            ax = plt.subplot(2, 3, i + 1)
            plt.imshow(images[cat_nums[i]].numpy().astype("uint8"))
            plt.title(class_names[labels[cat_nums[i]]])
            plt.axis("off")
        for i in range(3):
            ax = plt.subplot(2, 3, i + 4)
            plt.imshow(images[dog_nums[i]].numpy().astype("uint8"))
            plt.title(class_names[labels[dog_nums[i]]])
            plt.axis("off")

view_cats_dogs(train_dataset)
```

![dogs_cats.png](/images/dogs_cats.png)

The following code is necessary for fast data imports within the rest of the program

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

```

We check the frequencies of the labels in the data. The following code counts the number of pictures labeled as "dog" and similarly for "cat"

```python
number_of_cats=0
number_of_dogs=0

for images, labels in train_dataset.unbatch():
    if labels ==0:
        number_of_cats+=1
        
    else:
        number_of_dogs+=1
print(number_of_cats)
print(number_of_dogs)
```

We see that there is an equal amount of both at 1000. Hence, a baseline classifier that always guesses cat (or equivalently dog) will be about 50% accurate.
This is the benchmark that we want our models to do better than.

# Model 1

We train our first CNN model, choosing it to have three max pool, drop out, and convolution layers, as well as three dense layers and the one flatten layer.

The choices for the 2D convolution layers were picked through trial and error and the help of a few of my friends who have experience training CNNs. The dense layer at the end must be of size two given our task is binary classification.

```python
model1=models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(32, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(64, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Flatten(), # n^2 * 64 length vector 
    layers.Dense(64, activation='relu'), 
    layers.Dense(2)])
```

We next choose to optimize it using the compile function function in tensor flow and with the following standard choices for the optimizer, loss function, and 
optimization metric. 

```python
model1.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])

history=model1.fit(train_dataset, epochs=20, validation_data=validation_dataset)
```

We finally train our model on the training data extracted earlier, and test it against the validation data_set. We choose 20 epochs in order to to get a breadth for how accurate the model is

![model1.png](/images/model1.png)

We see that the model hovers around 55-65% accuracy.

# Model 2

We build off of the previous model but now use data_augmentation to try to improve performance (i.e. rotating and flipping the images), in order for our model to explore hidden characterstic layers present that differentiate between cats and dogs. The following code lets creates a function called ```data_augmentation``` which will be used to rotate and flip the images randomly in our CNN, and the effects are displayed of flipping/rotating in the following plots

```python
data_augmentation = tf.keras.Sequential([
  layers.RandomRotation(0.2)
])

plt.figure(figsize=(10, 10))
    
for image, label in train_dataset.take(1):
    plt.figure(figsize=(10, 10))
    first_image = image[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
        plt.imshow(augmented_image[0] / 255)
        plt.axis('off')
```

![rotate.png](/images/rotate.png)

```python
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip('horizontal')
])

plt.figure(figsize=(10, 10))
    
for image, label in train_dataset.take(1):
    plt.figure(figsize=(10, 10))
    first_image = image[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
        plt.imshow(augmented_image[0] / 255)
        plt.axis('off')
```
![flip.png](/images/flip.png)

We incorporate the full version of ```data_augmentation``` (which uses code to both rotate and flip) into our first model to create our second model

```python
model2=models.Sequential([
    data_augmentation,
    layers.Conv2D(32, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(32, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(64, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Flatten(), # n^2 * 64 length vector 
    layers.Dense(64, activation='relu'), 
    layers.Dense(2)])
```

The code used to optimize and train the model is identical to that of the first model, and gives us the following results

![model2.png](/images/model2.png)

We see that this model attains about 60-67% validation accuracy

# Model 3

We further build off of the previous model, but this time use a ```preprocessor``` function determined by the following code

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

We add this into the beginning of our second model to obtain the following

```python
model3=models.Sequential([
    preprocessor,
    data_augmentation,
    layers.Conv2D(32, (3, 3), activation='relu', input_shape= (160,160,3)),
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(32, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Conv2D(64, (3, 3), activation='relu'), 
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Flatten(), # n^2 * 64 length vector 
    layers.Dense(64, activation='relu'), 
    layers.Dense(2)])
```

It is optimized and trained with the same commands as before so we obtain the following results

![model3.png](/images/model3.png)

The model peeks at about 70-75% validation accuracy 


# Model 4

We move onto the fourth and final model, which is built on top of a previously used CNN called ```MobileNetV2```. The following code gives us the desired layer that we will throw into the model

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

We now create the model, which uses MovileNetV2 so it appears much more barebones compared to those earlier (as it doesn't have the convolutional layers that were personally designed since MovileNetV2 takes care of this)

```python
model4=models.Sequential([
    preprocessor,
    data_augmentation,
    base_model_layer,
    layers.MaxPooling2D(pool_size=(2, 2)), layers.Dropout(0.2), 
    layers.Dense(2)])
```

Unfortunately, the code above was running into several errors that couldn't be fixed in time of the deadline and so we were not able to display the results.

# Testing

As a result, it is clear from the displayed results that model 3 achieves the highest validation accuracy

We run it on our testing datasetand observe its performance:

```python
loss, accuracy = model3.evaluate(test_dataset)
print('Test accuracy of model 3:', accuracy)
```

![model3_acc.png](/images/model3_acc.png)
