---
layout: post
title: Fake News Classification with CNNs
---

In this post we will create a fake news classifier using CNNs using Tensorflow and make us of word-embeddings. 

We begin by doing our necessary imports and downloading the fake_news dataset

```python
import tensorflow as tf
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
import nltk
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import re
import string
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
from sklearn.decomposition import PCA
```

```python
url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df_train = pd.read_csv(url)
```

We also download the save the stopwords that we will use in the creation of our datasets

```python
nltk.download('stopwords')
```

We next write a function to create both our training and testing datasets. It works by changing the text to lowercase, then uses a lambda function to remove the stopwords, and finally create our dataset that has two inputs ('title' and 'text') and one output ('fake'). We also batch the data as recommended in the homework directions to allow for quicker training

```python
def make_dataset(df):
    
    df['title'] = df['title'].str.lower()
    df['text'] = df['text'].str.lower()
    
    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))
    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))

    
    dataset = tf.data.Dataset.from_tensor_slices(
    (
        {"title" : df[["title"]], 
            "text" : df[["text"]]}, 
        { "fake" : df[["fake"]]
        }
    )
    )
    dataset = data.batch(100)
    return dataset
```

We use this to create the training and split 20% of it to get validation data

```python
train_data = make_dataset(df_train)


train_size = int(0.8*len(train_data)) 

train = train_data.take(train_size)
val   = train_data.skip(train_size)
```

We calculate the base rate and the goal of our model is to achieve more than the base rate, which is about 52%

```python
base_rate= df_train['fake'].sum() / df_train.shape[0]
```

We next create our standardization function as given in the homework directions and use it for the text_vectorization that will be involved in the model training

```python
size_vocabulary=2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data) #turn words into all lower case
    no_punctuation = tf.strings.regex_replace(lowercase, #remove puncuation marks
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

We next create three different models for fake news classification. The first uses just the article titles, the second just the article text, and the last both.

For the first model, we create it as follows:

```python
title_input = keras.Input( #input layer
    shape = (1,),
    name = 'title',
    dtype = 'string'
)

title_features = title_vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "title_embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features) 
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dense(16, activation='relu')(title_features)

output = layers.Dense(2, name = 'fake')(title_features) 

model1 = keras.Model(
    inputs = title_input,
    outputs = output
)
```

We use a 3-dimensional embedding as a baseline and other layers to improve performance, with the last layer being of size 2 for the probability that an article is fake news or not

We train it in the same procedure used for homework 3, buth with 10 epochs for efficiency.

```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model1.fit(train,
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)

```

The following provides a visualization into the model's training:

```python
from tensorflow.keras import utils
utils.plot_model(model1)
```

![model1_visual.png](/images/model1_visual.png)

Which has a validation accuracy of more than 87%

![hw4_model1_results.png](/images/hw4_model1_results.png)

We next create our second model to handle the article text. The process is similar but uses our vectorize_text code from earlier. This time, we increase the dimension of word_embedding to 8

```python
vectorize_text = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary,
    output_mode='int',
    output_sequence_length=500) 

vectorize_text.adapt(train.map(lambda x, y: x["text"]))

text_input = keras.Input(
    shape = (1,),
    name = 'text',
    dtype = 'string'
)

text_features = vectorize_text(text_input) 
text_features = layers.Embedding(size_vocabulary, 8, name = "text_embedding")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dense(16, activation='relu')(text_features)

output = layers.Dense(2, name = 'fake')(text_features)

model2 = keras.Model(
    inputs = text_input,
    outputs = output
)
```

Training it in an identical way to model1, we see that model2 scores an even higher validation accuracy of 97.8%!

![hw4_model2_results.png](/images/hw4_model2_results.png)

We finally train our third model, which uses both titles and the text. This is done by concatentating both features from the earlier models as well as the inputs (note that as this point we increased the embedding dimension from our first model to match)

```python
combine = layers.concatenate([title_features, text_features], axis = 1) 
combine = layers.Dense(16, activation = 'relu')(combine) 

model3 = keras.Model(
    inputs = [title_input, text_input]
    outputs = layers.Dense(2, name = 'fake')(combine)
)

```

Doing this and training in the same way as prior gives a whopping validation accuracy of nearly 99%!

![hw4_model3_results.png](/images/hw4_model3_results.png)

From this, it is clear that using both the title and text gives the optimal accuracy of fake news classification. 

We also plot the training history of this model using the code from lecture:

![history_plot.png](/images/history_plot.png)

We finally run this model on our testing data. We import it and create is as follows (using our make_dataset function from earlier)

```python
df_test = pd.read_csv("https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true")

test = make_dataset(df_test)

model3.evaluate(test)
```
![hw4_model3_test.png](/images/hw4_model3_test.png)

where it clearly scores very well!

Furthermore, since our model made use of word embeddings, it is possible to visualize which words might have been commin in fake news titles. 

We first grab the weights from our embedding layer and the corresponding vocabulary. 

Recall from earlier that we set our embedding weights to have dimension 8 in model 3, so we reduce the weights to have 2 dimensions using PCA so that we can create a 2D visualization

```python
weights = model3.get_layer('title_embedding').get_weights()[0]
vocab = title_vectorize_layer.get_vocabulary()

pca = PCA(n_components = 2)
weights=pca.fit_transform(weights)
```

```python
embedding = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding[1:10]
```

![hw_4_table.png](/images/hw_4_table.png)

Plotting using plotly, we obtain

![hw_4_graph.png](/images/hw_4_graph.png)

From here, we can make out several embeddings by hovering over the data. Five are "Trump", "Hilary", "democrat", "Vietnam", and "Muslim" which one might expect to see as common key words in fake news. 